SRGAN超分辨模型

SRGAN模型训练与测试
1. 项目解读
2. 模型训练
3. 模型测试

本项目需要在 GPU 环境下运行，点击本页面最右边 < 进行切换

SRGAN超分辨模型
随着生成对抗网络GAN的发展，生成器和判别器的对抗学习机制在图像生成任务中展现出很强大的学习能力。Twitter的研究者们使用ResNet作为生成器结构，使用VGG作为判别器结构，提出了SRGAN模型，这是本次实践课使用的模型，其结构示意图如下：

Image
SRGAN模型
生成器结构包含了若干个不改变特征分辨率的残差模块和多个基于亚像素卷积的后上采样模块。

判别器结构则包含了若干个通道数不断增加的卷积层，每次特征通道数增加一倍时，特征分辨率降低为原来的一半。

SRGAN模型的损失函数包括两部分，内容损失与对抗损失。

l
S
R
=
l
S
R
X

 
content loss 
 
+
10
−
3
l
S
R
G
e
n

 
adversarial loss 
 

 
perceptual loss (for VGG based content losses) 
 
 
对抗损失就是标准的GAN损失，而内容损失则是基于VGG网络特征构建，它代替了之前SRCNN使用的MSE损失函数，如下：

ℓ
ϕ
,
j
feat 
(
^
y
,
y
)
=
1
C
j
H
j
W
j
∥
∥
ϕ
j
(
^
y
)
−
ϕ
j
(
y
)
∥
∥
2
2
 
SRGAN通过生成器和判别器的对抗学习取得了视觉感知上更好的重建结果。不过基于GAN的模型虽然可以取得好的超分结果，但是也往往容易放大噪声。

SRGAN模型训练与测试
1. 项目解读
下面我们首先来剖析整个项目的代码。

1.1 数据集和基准模型
首先我们来介绍使用的数据集和基准模型，大多数超分重建任务的数据集都是通过从高分辨率图像进行降采样获得，这里我们也采用这样的方案。数据集既可以选择ImageNet这样包含上百万图像的大型数据集，也可以选择模式足够丰富的小数据集，这里我们选择一个垂直领域的高清人脸数据集，CelebA-HQ。CelebA-HQ数据集发布于2019年，包含30000张包括不同属性的高清人脸图，其中图像大小均为1024×1024。

数据集放置在项目根目录的 dataset 目录下，包括两个子文件夹，train 和 val。

在项目开始之前需要加载数据集和预训练模型，加载方式如下图所示： 仅第一次使用时操作！！！

Image

由于数据集较大，加载需要较长时间，加载完毕后，会有弹窗显示

Image

数据下载成功后，会得到两个 zip 文件，一个是数据集(Face_SuperResolution_Dataset.zip)，一个是预训练模型(vgg16-397923af.zip)

Image

运行下方代码解压数据集，第一次使用时运行即可！！，当显示 10 个 . 时，代表解压完成。

1
!unzip -o Face_SuperResolution_Dataset.zip | awk 'BEGIN {ORS=" "} {if(NR%3000==0)print "."}'
. . . . . . . . . . 
解压完成后会得到一个 dataset 文件夹，其文件结构如下

dataset
    - train
    - val
运行下方代码解压预训练模型，第一次使用时运行即可！！

1
!mkdir checkpoints
2
!unzip -o vgg16-397923af.zip -d ./checkpoints/
Archive:  vgg16-397923af.zip

  inflating: ./hub/checkpoints/vgg16-397923af.pth  
1.2 数据集接口
下面我们从高分辨率图进行采样得到低分辨率图，然后组成训练用的图像对，核心代码如下：

1
from os import listdir
2
from os.path import join
3
​
4
from PIL import Image
5
from torch.utils.data.dataset import Dataset
6
from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize
7
​
8
​
9
def is_image_file(filename):
10
    return any(
11
        filename.endswith(extension)
12
        for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])
13
​
14
​
15
# 基于上采样因子对裁剪尺寸进行调整，使其为upscale_factor的整数倍
16
def calculate_valid_crop_size(crop_size, upscale_factor):
17
    return crop_size - (crop_size % upscale_factor)
18
​
19
​
20
# 训练集高分辨率图预处理函数
21
def train_hr_transform(crop_size):
22
    return Compose([
23
        RandomCrop(crop_size),
24
        ToTensor(),
25
    ])
26
​
27
​
28
# 训练集低分辨率图预处理函数
29
def train_lr_transform(crop_size, upscale_factor):
30
    return Compose([
31
        ToPILImage(),
32
        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),
33
        ToTensor()
34
    ])
35
​
36
​
37
def display_transform():
38
    return Compose([ToPILImage(), Resize(400), CenterCrop(400), ToTensor()])
39
​
40
​
41
# 训练数据集类
42
class TrainDatasetFromFolder(Dataset):
43
    def __init__(self, dataset_dir, crop_size, upscale_factor):
44
        super(TrainDatasetFromFolder, self).__init__()
45
        self.image_filenames = [
46
            join(dataset_dir, x) for x in listdir(dataset_dir)
47
            if is_image_file(x)
48
        ]  # 获得所有图像
49
        crop_size = calculate_valid_crop_size(crop_size,
50
                                              upscale_factor)  # 获得裁剪尺寸
51
        self.hr_transform = train_hr_transform(crop_size)  # 高分辨率图预处理函数
52
        self.lr_transform = train_lr_transform(crop_size,
53
                                               upscale_factor)  # 低分辨率图预处理函数
54
​
55
    # 数据集迭代指针
56
    def __getitem__(self, index):
57
        hr_image = self.hr_transform(Image.open(
58
            self.image_filenames[index]))  # 随机裁剪获得高分辨率图
59
        lr_image = self.lr_transform(hr_image)  # 获得低分辨率图
60
        return lr_image, hr_image
61
​
62
    def __len__(self):
63
        return len(self.image_filenames)
64
​
65
​
66
# 验证数据集类
67
class ValDatasetFromFolder(Dataset):
68
    def __init__(self, dataset_dir, upscale_factor):
69
        super(ValDatasetFromFolder, self).__init__()
70
        self.upscale_factor = upscale_factor
71
        self.image_filenames = [
72
            join(dataset_dir, x) for x in listdir(dataset_dir)
73
            if is_image_file(x)
74
        ]
75
​
76
    def __getitem__(self, index):
77
        hr_image = Image.open(self.image_filenames[index])
78
​
79
        # 获得图像窄边获得裁剪尺寸
80
        w, h = hr_image.size
81
        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)
82
        lr_scale = Resize(crop_size // self.upscale_factor,
83
                          interpolation=Image.BICUBIC)
84
        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)
85
        hr_image = CenterCrop(crop_size)(hr_image)  # 中心裁剪获得高分辨率图
86
        lr_image = lr_scale(hr_image)  # 获得低分辨率图
87
        hr_restore_img = hr_scale(lr_image)
88
        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(
89
            hr_image)
90
​
91
    def __len__(self):
92
        return len(self.image_filenames)
93
​
94
​
95
class TestDatasetFromFolder(Dataset):
96
    def __init__(self, dataset_dir, upscale_factor):
97
        super(TestDatasetFromFolder, self).__init__()
98
        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'
99
        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'
100
        self.upscale_factor = upscale_factor
101
        self.lr_filenames = [
102
            join(self.lr_path, x) for x in listdir(self.lr_path)
103
            if is_image_file(x)
104
        ]
105
        self.hr_filenames = [
106
            join(self.hr_path, x) for x in listdir(self.hr_path)
107
            if is_image_file(x)
108
        ]
109
​
110
    def __getitem__(self, index):
111
        image_name = self.lr_filenames[index].split('/')[-1]
112
        lr_image = Image.open(self.lr_filenames[index])
113
        w, h = lr_image.size
114
        hr_image = Image.open(self.hr_filenames[index])
115
        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w),
116
                          interpolation=Image.BICUBIC)
117
        hr_restore_img = hr_scale(lr_image)
118
        return image_name, ToTensor()(lr_image), ToTensor()(
119
            hr_restore_img), ToTensor()(hr_image)
120
​
121
    def __len__(self):
122
        return len(self.lr_filenames)
从上述代码可以看出，包含了两个预处理函数接口，分别是train_hr_transform，train_lr_transform。train_hr_transform包含的操作主要是随机裁剪，而train_lr_transform包含的操作主要是缩放。

另外还有一个函数calculate_valid_crop_size，对于训练集来说，它用于当配置的图像尺寸crop_size不能整除上采样因子upscale_factor时对crop_size进行调整，我们在使用的时候应该避免这一点，即配置crop_size让它等于upscale_factor的整数倍。对于验证集，图像的窄边min(w, h)会被用于crop_size的初始化，所以该函数的作用是当图像的窄边不能整除上采样因子upscale_factor时对crop_size进行调整。

训练集类TrainDatasetFromFolder包含了若干操作，它使用train_hr_transform从原图像中随机裁剪大小为裁剪尺寸的正方形的图像，使用train_lr_transform获得对应的低分辨率图。而验证集类ValDatasetFromFolder则将图像按照调整后的crop_size进行中心裁剪，然后使用train_lr_transform获得对应的低分辨率图。

在这里我们只使用了随机裁剪作为训练时的数据增强操作，实际训练工程项目时，应该根据需要添加多种数据增强操作才能获得泛化能力更好的模型。

1.3 生成器
生成器是一个基于残差模块的上采样模型，它的定义包括残差模块，上采样模块以及主干模型，如下：

ResidualBlock
1
import math
2
import torch
3
from torch import nn
4
​
5
​
6
# 生成模型
7
class Generator(nn.Module):
8
    def __init__(self, scale_factor):
9
        upsample_block_num = int(math.log(scale_factor, 2))
10
​
11
        super(Generator, self).__init__()
12
        # 第一个卷积层，卷积核大小为9×9，输入通道数为3，输出通道数为64
13
        self.block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=9, padding=4),
14
                                    nn.PReLU())
15
        # 6个残差模块
16
        self.block2 = ResidualBlock(64)
17
        self.block3 = ResidualBlock(64)
18
        self.block4 = ResidualBlock(64)
19
        self.block5 = ResidualBlock(64)
20
        self.block6 = ResidualBlock(64)
21
        self.block7 = nn.Sequential(
22
            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64))
23
        # upsample_block_num个上采样模块，每一个上采样模块恢复2倍的上采样倍率
24
        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]
25
        # 最后一个卷积层，卷积核大小为9×9，输入通道数为64，输出通道数为3
26
        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))
27
        self.block8 = nn.Sequential(*block8)
28
​
29
    def forward(self, x):
30
        block1 = self.block1(x)
31
        block2 = self.block2(block1)
32
        block3 = self.block3(block2)
33
        block4 = self.block4(block3)
34
        block5 = self.block5(block4)
35
        block6 = self.block6(block5)
36
        block7 = self.block7(block6)
37
        block8 = self.block8(block1 + block7)
38
​
39
        return (torch.tanh(block8) + 1) / 2
40
​
41
​
42
# 残差模块
43
class ResidualBlock(nn.Module):
44
    def __init__(self, channels):
45
        super(ResidualBlock, self).__init__()
46
        # 两个卷积层，卷积核大小为3×3，通道数不变
47
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
48
        self.bn1 = nn.BatchNorm2d(channels)
49
        self.prelu = nn.PReLU()
50
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
51
        self.bn2 = nn.BatchNorm2d(channels)
52
​
53
    def forward(self, x):
54
        residual = self.conv1(x)
55
        residual = self.bn1(residual)
56
        residual = self.prelu(residual)
57
        residual = self.conv2(residual)
58
        residual = self.bn2(residual)
59
​
60
        return x + residual
61
​
62
​
63
# 上采样模块，每一个恢复分辨率为2
64
class UpsampleBLock(nn.Module):
65
    def __init__(self, in_channels, up_scale):
66
        super(UpsampleBLock, self).__init__()
67
        # 卷积层，输入通道数为in_channels，输出通道数为in_channels * up_scale ** 2
68
        self.conv = nn.Conv2d(in_channels,
69
                              in_channels * up_scale**2,
70
                              kernel_size=3,
71
                              padding=1)
72
        # PixelShuffle上采样层，来自于后上采样结构
73
        self.pixel_shuffle = nn.PixelShuffle(up_scale)
74
        self.prelu = nn.PReLU()
75
​
76
    def forward(self, x):
77
        x = self.conv(x)
78
        x = self.pixel_shuffle(x)
79
        x = self.prelu(x)
80
        return x
在上述的生成器定义中，调用了nn.PixelShuffle模块来实现上采样，它的具体原理在上节基于亚像素卷积的后上采样ESPCN模型中有详细介绍。

1.4 判别器
判别器是一个普通的类似于VGG的CNN模型，完整定义如下：

1
# 残差模块
2
class Discriminator(nn.Module):
3
    def __init__(self):
4
        super(Discriminator, self).__init__()
5
        self.net = nn.Sequential(
6
            # 第1个卷积层，卷积核大小为3×3，输入通道数为3，输出通道数为64
7
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
8
            nn.LeakyReLU(0.2),
9
            # 第2个卷积层，卷积核大小为3×3，输入通道数为64，输出通道数为64
10
            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),
11
            nn.BatchNorm2d(64),
12
            nn.LeakyReLU(0.2),
13
            # 第3个卷积层，卷积核大小为3×3，输入通道数为64，输出通道数为128
14
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
15
            nn.BatchNorm2d(128),
16
            nn.LeakyReLU(0.2),
17
            # 第4个卷积层，卷积核大小为3×3，输入通道数为128，输出通道数为128
18
            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),
19
            nn.BatchNorm2d(128),
20
            nn.LeakyReLU(0.2),
21
            # 第5个卷积层，卷积核大小为3×3，输入通道数为128，输出通道数为256
22
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
23
            nn.BatchNorm2d(256),
24
            nn.LeakyReLU(0.2),
25
            # 第6个卷积层，卷积核大小为3×3，输入通道数为256，输出通道数为256
26
            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),
27
            nn.BatchNorm2d(256),
28
            nn.LeakyReLU(0.2),
29
            # 第7个卷积层，卷积核大小为3×3，输入通道数为256，输出通道数为512
30
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
31
            nn.BatchNorm2d(512),
32
            nn.LeakyReLU(0.2),
33
            # 第8个卷积层，卷积核大小为3×3，输入通道数为512，输出通道数为512
34
            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),
35
            nn.BatchNorm2d(512),
36
            nn.LeakyReLU(0.2),
37
            # 全局池化层
38
            nn.AdaptiveAvgPool2d(1),
39
            # 两个全连接层，使用卷积实现
40
            nn.Conv2d(512, 1024, kernel_size=1),
41
            nn.LeakyReLU(0.2),
42
            nn.Conv2d(1024, 1, kernel_size=1))
43
​
44
    def forward(self, x):
45
        batch_size = x.size(0)
46
        return torch.sigmoid(self.net(x).view(batch_size))
1.5 损失定义
1
import torch
2
from torch import nn
3
from torchvision.models.vgg import vgg16
4
import os
5
os.environ['TORCH_HOME'] = './'
6
​
7
​
8
# 生成器损失定义
9
class GeneratorLoss(nn.Module):
10
    def __init__(self):
11
        super(GeneratorLoss, self).__init__()
12
        vgg = vgg16(pretrained=True)
13
        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()
14
        for param in loss_network.parameters():
15
            param.requires_grad = False
16
        self.loss_network = loss_network
17
        self.mse_loss = nn.MSELoss()  # MSE损失
18
        self.tv_loss = TVLoss()  # TV平滑损失
19
​
20
    def forward(self, out_labels, out_images, target_images):
21
        # 对抗损失
22
        adversarial_loss = torch.mean(1 - out_labels)
23
        # 感知损失
24
        perception_loss = self.mse_loss(self.loss_network(out_images),
25
                                        self.loss_network(target_images))
26
        # 图像MSE损失
27
        image_loss = self.mse_loss(out_images, target_images)
28
        # TV平滑损失
29
        tv_loss = self.tv_loss(out_images)
30
        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss
31
​
32
​
33
# TV平滑损失
34
class TVLoss(nn.Module):
35
    def __init__(self, tv_loss_weight=1):
36
        super(TVLoss, self).__init__()
37
        self.tv_loss_weight = tv_loss_weight
38
​
39
    def forward(self, x):
40
        batch_size = x.size()[0]
41
        h_x = x.size()[2]
42
        w_x = x.size()[3]
43
        count_h = self.tensor_size(x[:, :, 1:, :])
44
        count_w = self.tensor_size(x[:, :, :, 1:])
45
        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()
46
        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()
47
        return self.tv_loss_weight * 2 * (h_tv / count_h +
48
                                          w_tv / count_w) / batch_size
49
​
50
    @staticmethod
51
    def tensor_size(t):
52
        return t.size()[1] * t.size()[2] * t.size()[3]
53
​
54
​
55
if __name__ == "__main__":
56
    g_loss = GeneratorLoss()
57
    print(g_loss)
GeneratorLoss(

  (loss_network): Sequential(

    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (1): ReLU(inplace=True)

    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (3): ReLU(inplace=True)

    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (6): ReLU(inplace=True)

    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (8): ReLU(inplace=True)

    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (11): ReLU(inplace=True)

    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (13): ReLU(inplace=True)

    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (15): ReLU(inplace=True)

    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (18): ReLU(inplace=True)

    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (20): ReLU(inplace=True)

    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (22): ReLU(inplace=True)

    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (25): ReLU(inplace=True)

    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (27): ReLU(inplace=True)

    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    (29): ReLU(inplace=True)

    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)

  )

  (mse_loss): MSELoss()

  (tv_loss): TVLoss()

)
生成器损失总共包含4部分，分别是对抗网络损失，逐像素的图像MSE损失，基于VGG模型的感知损失，用于约束图像平滑的TV平滑损失。

2. 模型训练
接下来我们来解读模型的核心训练代码，查看模型训练的结果。训练代码除了模型和损失定义，还需要完成优化器定义，训练和验证指标变量的存储，核心代码如下：

1
from math import exp
2
​
3
import torch
4
import torch.nn.functional as F
5
from torch.autograd import Variable
6
​
7
​
8
def gaussian(window_size, sigma):
9
    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
10
    return gauss / gauss.sum()
11
​
12
​
13
def create_window(window_size, channel):
14
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
15
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
16
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
17
    return window
18
​
19
​
20
def _ssim(img1, img2, window, window_size, channel, size_average=True):
21
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)
22
    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)
23
​
24
    mu1_sq = mu1.pow(2)
25
    mu2_sq = mu2.pow(2)
26
    mu1_mu2 = mu1 * mu2
27
​
28
    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
29
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
30
    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2
31
​
32
    C1 = 0.01 ** 2
33
    C2 = 0.03 ** 2
34
​
35
    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
36
​
37
    if size_average:
38
        return ssim_map.mean()
39
    else:
40
        return ssim_map.mean(1).mean(1).mean(1)
41
​
42
​
43
class SSIM(torch.nn.Module):
44
    def __init__(self, window_size=11, size_average=True):
45
        super(SSIM, self).__init__()
46
        self.window_size = window_size
47
        self.size_average = size_average
48
        self.channel = 1
49
        self.window = create_window(window_size, self.channel)
50
​
51
    def forward(self, img1, img2):
52
        (_, channel, _, _) = img1.size()
53
​
54
        if channel == self.channel and self.window.data.type() == img1.data.type():
55
            window = self.window
56
        else:
57
            window = create_window(self.window_size, channel)
58
​
59
            if img1.is_cuda:
60
                window = window.cuda(img1.get_device())
61
            window = window.type_as(img1)
62
​
63
            self.window = window
64
            self.channel = channel
65
​
66
        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)
67
​
68
​
69
def ssim(img1, img2, window_size=11, size_average=True):
70
    (_, channel, _, _) = img1.size()
71
    window = create_window(window_size, channel)
72
​
73
    if img1.is_cuda:
74
        window = window.cuda(img1.get_device())
75
    window = window.type_as(img1)
76
​
77
    return _ssim(img1, img2, window, window_size, channel, size_average)
78
​
创建一些文件夹，首次使用时运行！！！

1
!mkdir training_results
2
!mkdir epochs
3
!mkdir statistics
mkdir: cannot create directory ‘training_results’: File exists

mkdir: cannot create directory ‘epochs’: File exists

mkdir: cannot create directory ‘statistics’: File exists
注意：由于阿里云平台 GPU 资源受限，本项目仅使用少量数据集进行训练

1
import os
2
from math import log10
3
​
4
import pandas as pd
5
import torch.optim as optim
6
import torch.utils.data
7
import torchvision.utils as utils
8
from torch.autograd import Variable
9
from torch.utils.data import DataLoader
10
from tqdm import tqdm
11
​
12
​
13
​
14
​
15
if __name__ == '__main__':
16
    
17
    CROP_SIZE = 240 #opt.crop_size   ## 裁剪尺寸，即训练尺度
18
    UPSCALE_FACTOR = 4#opt.upscale_factor  ## 超分上采样倍率
19
    NUM_EPOCHS = 20  #opt.num_epochs  ## 迭代epoch次数
20
    
21
    ## 获取训练集/验证集
22
    train_set = TrainDatasetFromFolder('dataset/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)
23
    val_set = ValDatasetFromFolder('dataset/val', upscale_factor=UPSCALE_FACTOR)
24
    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=16, shuffle=True)
25
    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)
26
  
27
    netG = Generator(UPSCALE_FACTOR) ##生成器定义
28
    netD = Discriminator() ##判别器定义
29
    generator_criterion = GeneratorLoss() ##生成器优化目标
30
    
31
    ## 是否使用GPU
32
    if torch.cuda.is_available():
33
        netG.cuda()
34
        netD.cuda()
35
        generator_criterion.cuda()
36
    
37
    ##生成器和判别器优化器
38
    optimizerG = optim.Adam(netG.parameters())
39
    optimizerD = optim.Adam(netD.parameters())
40
    
41
    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}
42
    ## epoch迭代
43
    for epoch in range(1, NUM_EPOCHS + 1):
44
        train_bar = tqdm(train_loader)
45
        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0} ##结果变量
46
    
47
        netG.train() ##生成器训练
48
        netD.train() ##判别器训练
49
​
50
        ## 每一个epoch的数据迭代
51
        for data, target in train_bar:
52
            g_update_first = True
53
            batch_size = data.size(0)
54
            running_results['batch_sizes'] += batch_size
55
    
56
            ## 优化判别器，最大化D(x)-1-D(G(z))
57
            real_img = Variable(target)
58
            if torch.cuda.is_available():
59
                real_img = real_img.cuda()
60
            z = Variable(data)
61
            if torch.cuda.is_available():
62
                z = z.cuda()
63
            fake_img = netG(z) ##获取生成结果
64
            netD.zero_grad()
65
            real_out = netD(real_img).mean()
66
            fake_out = netD(fake_img).mean()
67
            d_loss = 1 - real_out + fake_out
68
            d_loss.backward(retain_graph=True)
69
            optimizerD.step() ##优化判别器
70
    
71
            ## 优化生成器 最小化1-D(G(z)) + Perception Loss + Image Loss + TV Loss
72
            netG.zero_grad()
73
            g_loss = generator_criterion(fake_out, fake_img, real_img)
74
            g_loss.backward()
75
            
76
            fake_img = netG(z)
77
            fake_out = netD(fake_img).mean()
78
            optimizerG.step()
79
​
80
            # 记录当前损失
81
            running_results['g_loss'] += g_loss.item() * batch_size
82
            running_results['d_loss'] += d_loss.item() * batch_size
83
            running_results['d_score'] += real_out.item() * batch_size
84
            running_results['g_score'] += fake_out.item() * batch_size
85
            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (
86
                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],
87
                running_results['g_loss'] / running_results['batch_sizes'],
88
                running_results['d_score'] / running_results['batch_sizes'],
89
                running_results['g_score'] / running_results['batch_sizes']))
90
        
91
        ## 对验证集进行验证
92
        netG.eval() ## 设置验证模式
93
        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'
94
        if not os.path.exists(out_path):
95
            os.makedirs(out_path)
96
        
97
        ## 计算验证集相关指标
98
        with torch.no_grad():
99
            val_bar = tqdm(val_loader)
100
            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}
101
            val_images = []
102
            for val_lr, val_hr_restore, val_hr in val_bar:
103
                batch_size = val_lr.size(0)
104
                valing_results['batch_sizes'] += batch_size
105
                lr = val_lr ##低分辨率真值图
106
                hr = val_hr ##高分辨率真值图
107
                if torch.cuda.is_available():
108
                    lr = lr.cuda() 
109
                    hr = hr.cuda()
110
                sr = netG(lr) ##超分重建结果
111
            
112
                batch_mse = ((sr - hr) ** 2).data.mean() ##计算MSE指标
113
                valing_results['mse'] += batch_mse * batch_size
114
                valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes'])) ##计算PSNR指标
115
                batch_ssim = ssim(sr, hr).item() ##计算SSIM指标
116
                valing_results['ssims'] += batch_ssim * batch_size
117
                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']
118
        ## 存储模型参数
119
        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))
120
        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))
121
        ## 记录训练集损失以及验证集的psnr,ssim等指标 \scores\psnr\ssim
122
        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])
123
        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])
124
        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])
125
        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])
126
        results['psnr'].append(valing_results['psnr'])
127
        results['ssim'].append(valing_results['ssim'])
128
        
129
        ## 存储结果到本地文件
130
        if epoch % 10 == 0 and epoch != 0:
131
            out_path = 'statistics/'
132
            data_frame = pd.DataFrame(
133
                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],
134
                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},
135
                index=range(1, epoch + 1))
136
            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')
137
​
[1/20] Loss_D: 0.9206 Loss_G: 0.0191 D(x): 0.3966 D(G(z)): 0.2900: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]

100%|██████████| 85/85 [00:01<00:00, 45.18it/s]

[2/20] Loss_D: 0.9882 Loss_G: 0.0099 D(x): 0.3624 D(G(z)): 0.3444: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.75it/s]

[3/20] Loss_D: 0.9785 Loss_G: 0.0087 D(x): 0.3593 D(G(z)): 0.3436: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]

100%|██████████| 85/85 [00:01<00:00, 45.57it/s]

[4/20] Loss_D: 1.0011 Loss_G: 0.0070 D(x): 0.5323 D(G(z)): 0.5201: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.72it/s]

[5/20] Loss_D: 0.9912 Loss_G: 0.0071 D(x): 0.3812 D(G(z)): 0.3706: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.98it/s]

[6/20] Loss_D: 0.9738 Loss_G: 0.0072 D(x): 0.4276 D(G(z)): 0.3970: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.68it/s]

[7/20] Loss_D: 1.0016 Loss_G: 0.0066 D(x): 0.1955 D(G(z)): 0.1928: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.08it/s]

[8/20] Loss_D: 1.0028 Loss_G: 0.0064 D(x): 0.1531 D(G(z)): 0.1508: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]

100%|██████████| 85/85 [00:01<00:00, 45.72it/s]

[9/20] Loss_D: 1.0018 Loss_G: 0.0066 D(x): 0.0594 D(G(z)): 0.0610: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.19it/s]

[10/20] Loss_D: 0.9963 Loss_G: 0.0061 D(x): 0.0795 D(G(z)): 0.0764: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.63it/s]

[11/20] Loss_D: 1.0042 Loss_G: 0.0061 D(x): 0.1649 D(G(z)): 0.1674: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.91it/s]

[12/20] Loss_D: 0.9918 Loss_G: 0.0058 D(x): 0.2955 D(G(z)): 0.2907: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.34it/s]

[13/20] Loss_D: 1.0028 Loss_G: 0.0056 D(x): 0.2586 D(G(z)): 0.2455: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.59it/s]

[14/20] Loss_D: 1.0006 Loss_G: 0.0057 D(x): 0.1642 D(G(z)): 0.1645: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.12it/s]

[15/20] Loss_D: 0.9968 Loss_G: 0.0057 D(x): 0.2240 D(G(z)): 0.2179: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.46it/s]

[16/20] Loss_D: 1.0059 Loss_G: 0.0055 D(x): 0.1927 D(G(z)): 0.1983: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.51it/s]

[17/20] Loss_D: 0.9990 Loss_G: 0.0057 D(x): 0.2181 D(G(z)): 0.2166: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.56it/s]

[18/20] Loss_D: 0.9990 Loss_G: 0.0051 D(x): 0.2205 D(G(z)): 0.2180: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.73it/s]

[19/20] Loss_D: 1.0063 Loss_G: 0.0051 D(x): 0.2115 D(G(z)): 0.2121: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 45.92it/s]

[20/20] Loss_D: 0.9974 Loss_G: 0.0050 D(x): 0.1125 D(G(z)): 0.1072: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]

100%|██████████| 85/85 [00:01<00:00, 44.92it/s]
从上述代码可以看出，训练时采用的crop_size为240×240，批处理大小为16，使用的优化器为Adam，Adam采用了默认的优化参数。

损失等相关数据将生成在 statistics 文件夹下。

上采样倍率为4的模型训练结果如下：

Image
4倍上采样的PSNR和SSIM曲线
3. 模型测试
接下来我们进行模型的测试。

3.1 测试代码
首先解读测试代码，需要完成模型的载入，图像预处理和结果存储，完整代码如下：

1
import torch
2
from PIL import Image
3
from torch.autograd import Variable
4
from torchvision.transforms import ToTensor, ToPILImage
5
​
6
​
7
UPSCALE_FACTOR = 4 ##上采样倍率
8
TEST_MODE = True ## 使用GPU进行测试
9
​
10
IMAGE_NAME = "./dataset/val/10879.jpg"  # 测试图片路径
11
​
12
MODEL_NAME = './epochs/netG_epoch_4_20.pth' ##模型路径
13
model = Generator(UPSCALE_FACTOR).eval() ##设置验证模式
14
if TEST_MODE:
15
    model.cuda()
16
    model.load_state_dict(torch.load(MODEL_NAME))
17
else:
18
    model.load_state_dict(torch.load(MODEL_NAME, map_location=lambda storage, loc: storage))
19
​
20
image = Image.open(IMAGE_NAME) ##读取图片
21
image = Variable(ToTensor()(image), volatile=True).unsqueeze(0) ##图像预处理
22
if TEST_MODE:
23
    image = image.cuda()
24
​
25
with torch.no_grad():
26
    RESULT_NAME = "out_srf_" + str(UPSCALE_FACTOR) + "_" + IMAGE_NAME.split("/")[-1]
27
    out = model(image)
28
    out_img = ToPILImage()(out[0].data.cpu())
29
    out_img.save(RESULT_NAME)
30
​
/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
预测结果将在本级目录生成，以 out_srf_ 开头

3.2 重建结果
下图展示了若干图片的超分辨结果。

第一行为使用双线性插值进行上采样的结果, 第二行为4倍超分结果，第三行为原始大图。

Image

本次我们对SRGAN模型进行了实践，使用高清人脸数据集进行训练，对低分辨率的人脸图像进行了超分重建，验证了SRGAN模型的有效性，不过该模型仍然有较大的改进空间，它需要使用成对数据集进行训练，而训练时低分辨率图片的模式产生过于简单，无法对复杂的退化类型完成重建。

当要对退化类型更加复杂的图像进行超分辨重建时，模型训练时也应该采取多种对应的数据增强方法，包括但不限于对比度增强，各类噪声污染，JPEG压缩失真等操作，这些就留给读者去做更多的实验。
